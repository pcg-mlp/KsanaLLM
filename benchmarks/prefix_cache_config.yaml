# 全局设置
setting:
  # 全局配置
  global:
    # TP并行
    tensor_para_size: 4
    # PP并行
    pipeline_para_size: 1
    # 是否启用lora适配，false表示不启用，true表示启用
    enable_lora_adapter: false
  # 调度相关配置
  batch_scheduler:
    # 请求在队列中的超时时间，单位为毫秒
    waiting_timeout_in_ms: 3600000
    # 请求队列的最大等待长度，超过对应长度时会丢弃新请求，主要用于流控。
    max_waiting_queue_len: 1200
    # 一个调度step处理的最大token个数，一次context decode视为多个token。
    max_token_number: 10240
    # 每一步推理时的最大batch_size
    max_batch_size: 10
    # 最大输入输出长度之和, 相当于prompt_len+max_new_tokens
    max_token_len: 2048
    # 单个请求可用blck小于该值触发换出
    swapout_block_threshold: 1.0
    # 单个请求可用block大于该值触发换入
    swapin_block_threshold: 2.0
    # 单个请求可用block大于该值拉起新任务
    launch_block_threshold: 2.0
    # 用于执行异步swap的线程池大小
    swap_threadpool_size: 8
  # 显存块相关配置
  block_manager:
    # 单个block可处理的最大token个数
    block_token_num: 16
    # 保留的显存占比，单位为百分比。
    reserved_device_memory_ratio: 0.01
    # Lora权重的显存占比
    lora_deivce_memory_ratio: 0.0
    # Lora权重在host上的预分配大小，相比device上的倍数
    lora_host_memory_factor: 10.0
    # block部分的显存占比，<0表示使用所有剩余显存
    block_device_memory_ratio: -1.0
    # block部分在host上的预分配大小，相比device上的倍数
    block_host_memory_factor: 5.0
    # prefix cache长度，即每个request共享多少个prefix token。
    # 默认为0表示不启用prefix cache。
    # 正整数为指定个数prefix tokens会被cache起来。
    # 若设置的值不能整除block_token_num，将会选取比设置值小但是能整除block_token_num的一个值。
    # 例如设置了45，将会调整到32作为实际生效的值。
    # 目前策略是如果这不能整取，则存在block的浪费，所以暂时选择向下回退的策略。
    # -1表示自动适配，自动适配暂时还没实现。
    prefix_cache_len: 910
  # 性能统计相关配置
  profiler:
    # 统计周期，每隔多少秒输出一次统计数据
    stat_interval_second: 5
    # 缓存队列大小
    stat_buffer_size: 1024
    # 异步上报的线程池大小
    report_threadpool_size: 4
# 模型配置
model_spec:
  # 基础模型配置，只能有一个
  base_model:
    # 模型路径
    model_dir: /model/llama-ft/13B/4-gpu
  # lora模型配置，可以有多个
  lora_models:
      # lora模型名称
    - model_name: lora_1
      # lora模型路径
      model_dir: /model/lora/lora_1/13B/2-gpu
